{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Call Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, merge,TimeDistributed,concatenate,Bidirectional,BatchNormalization,Add,AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import metrics\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras_bert import get_custom_objects\n",
    "from tqdm import tqdm\n",
    "from chardet import detect\n",
    "import keras\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import Tokenizer as k_Tokenizer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,LabelBinarizer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,confusion_matrix,auc\n",
    "from sklearn.utils import class_weight\n",
    "from  sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import os, argparse,cv2,h5py,string\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "import codecs\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage import data\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto,InteractiveSession\n",
    "from keras.backend.tensorflow_backend import set_session,clear_session,get_session\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.per_process_gpu_memory_fraction =.6\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.visible_device_list = \"1\"\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = .3\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "# G =tf.Graph()\n",
    "# sess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))\n",
    "# sess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))\n",
    "\n",
    "\n",
    "\n",
    "# Seed value\n",
    "\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 2019\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "\n",
    "# Declare the two file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./Modality')  ### set working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Call the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This files contains the Image ID, Related Question and Corrospoding answers. Each file contains training data (first 3200), validation data (next 500) and testing data (rest 125)\n",
    "\n",
    "######################## If we do analysis for \"Modality\" category ############################\n",
    "\n",
    "\n",
    "filename_mod=\"/home/local/AD/asarkar2/Question/C1_Modality_all.txt\"\n",
    "filename_glove=\"/home/local/AD/asarkar2/glove.6B.100d.txt\"     ### golve vector\n",
    "\n",
    "# filename_organ=\"/home/local/AD/asarkar2/Question/C3_Organ_all.txt\"\n",
    "# filename_plane=\"/home/local/AD/asarkar2/Question/C2_plane_all.txt\"\n",
    "\n",
    "### This files are uploaded to \"Data file\" in google drive\n",
    "### Data File------> All Question  (Location)\n",
    "\n",
    "####################### Image Path #################################################################\n",
    "\n",
    "### This files are uploaded to \"Data file\" in google drive\n",
    "### Data File------> All Image  (Location)\n",
    "\n",
    "img_path_train=\"/home/local/AD/asarkar2/Image File/VQAMed2019_Train_Images/VQAMed2019_Train_Images\"\n",
    "img_path_valid=\"/home/local/AD/asarkar2/Image File/VQAMed2019_Valid_Images/VQAMed2019_Valid_Images\"\n",
    "img_path_test=\"/home/local/AD/asarkar2/Image File/VQAMed2019_Test_Images/VQAMed2019_Test_Images\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Call a \"separate\" function to separate image id, question and answer from above file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model_util.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id=separate(filename_mod).get(\"Image_id\")\n",
    "question=separate(filename_mod).get(\"Question\")\n",
    "answer=separate(filename_mod).get(\"Answer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 3200 elements are from train data\n",
    "# 3200-3700 elements are from valid data\n",
    "# rest are from test data\n",
    "\n",
    "image_id_train=img_id[:3200]\n",
    "image_id_valid=img_id[3200:3700]\n",
    "image_id_test=img_id[3700:]\n",
    "\n",
    "\n",
    "question_train=question[:3200]\n",
    "question_valid=question[3200:3700]\n",
    "question_test=question[3700:]\n",
    "\n",
    "answer_train=answer[:3200]\n",
    "answer_valid=answer[3200:3700]\n",
    "answer_test=answer[3700:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call image_preprocess function to process the image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=image_preprocess(img_path_train,image_id_train)\n",
    "valid_image=image_preprocess(img_path_valid,image_id_valid)\n",
    "test_image=image_preprocess(img_path_test,image_id_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image=np.load(\"./Image_array/modality_train_image.npy\")\n",
    "# valid_image=np.load(\"./Image_array/modality_valid_image.npy\")\n",
    "# test_image=np.load(\"./Image_array/modality_test_image.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Standardized the image pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_std=train_image/255\n",
    "valid_image_std=valid_image/255\n",
    "test_image_std=test_image/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Call label_preprocess Class to pre-process the answer (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################# Integer Label Encoding ##########################################\n",
    "\n",
    "integer_label_train=LabelPreprocess.integer_encoding(answer_train,answer[:3700])\n",
    "integer_label_valid=LabelPreprocess.integer_encoding(answer_valid,answer[:3700])\n",
    "integer_label_test=LabelPreprocess.integer_encoding(answer_test,answer[:3700])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# One-Hot Label Encoding ##########################################\n",
    "\n",
    "onehot_label_train=LabelPreprocess.onehot_encoding(answer_train,answer[:3700])\n",
    "onehot_label_valid=LabelPreprocess.onehot_encoding(answer_valid,answer[:3700])\n",
    "onehot_label_test=LabelPreprocess.onehot_encoding(answer_test,answer[:3700])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BERT model question pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ques_bert,valid_ques_bert,test_ques_bert=bert_question_preprocess(12,'/home/local/AD/asarkar2/BERT',question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LSTM model question pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## sequence of the token for question ############################################\n",
    "\n",
    "seq_train_question=QuestionPreprocess.sequence_question(question_train,question_train)\n",
    "seq_valid_question=QuestionPreprocess.sequence_question(question_valid,question_train)\n",
    "seq_test_question=QuestionPreprocess.sequence_question(question_test,question_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Padding question ########################################################\n",
    "\n",
    "# maxlength of each question for modality-----> 11\n",
    "# maxlength of each question for organ-----> 10\n",
    "# maxlength of each question for modality-----> 9\n",
    "\n",
    "\n",
    "padding_train_ques=QuestionPreprocess.padding_question(seq_train_question,maxlength=11,padding_criterion=\"post\")\n",
    "padding_valid_ques=QuestionPreprocess.padding_question(seq_valid_question,maxlength=11,padding_criterion=\"post\")\n",
    "padding_test_ques=QuestionPreprocess.padding_question(seq_test_question,maxlength=11,padding_criterion=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_test_ques.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Embedding Matrix for Glove Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(glove_filename,token_data): \n",
    "    word_index=token_data # call my_token function\n",
    "    embeddings_index = dict()\n",
    "    f = open(glove_filename,encoding='utf8') # call glove vector text file\n",
    "    for line in f:\n",
    "        values=line.split() # split the each line in text file\n",
    "        word = values[0] # first index associate with word and othe other indexs represent embedding vector associated with that word \n",
    "        coefs = np.asarray(values[1:], dtype='float32') \n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    vocab_size=len(token_data)\n",
    "    embedding_matrix=np.zeros((vocab_size+1,100)) # define embedding matrix\n",
    "    print(word_index.items())\n",
    "    for word,i in word_index.items():                # create our embedding matrix for each word of questions.\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_data=QuestionPreprocess.tokenize_question(question_train)\n",
    "\n",
    "emb_mat=embedding(filename_glove,token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run vgg_model_baseline.ipynb\n",
    "%run resnet_model_baseline.ipynb\n",
    "%run lstm_text_model.ipynb\n",
    "%run bert_text_model.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### VGG-16 & LSTM ####################\n",
    "\n",
    "vgg=vgg_image_model()\n",
    "lstm=lstm_text_model(emb_mat,emb_mat.shape[0],padding_train_ques.shape[1])\n",
    "\n",
    "combined = Concatenate(axis=1)([lstm.output,vgg.output])\n",
    "c1=Dense(2048,activation=\"relu\")(combined)\n",
    "c2=Dense(1000,activation=\"relu\")(c1)\n",
    "c3=Dense(onehot_label_train.shape[1])(c2)\n",
    "c4=Activation(\"softmax\")(c3)\n",
    "\n",
    "\n",
    "model_vqa = Model(inputs=[lstm.input,vgg.input], outputs=c4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### VGG-16 & BERT ####################\n",
    "\n",
    "vgg=vgg_image_model()\n",
    "bert=bert_text_model(12,'/home/local/AD/asarkar2/BERT')\n",
    "\n",
    "combined = Concatenate(axis=1)([bert.output,vgg.output])\n",
    "c1=Dense(2048,activation=\"relu\")(combined)\n",
    "c2=Dense(1000,activation=\"relu\")(c1)\n",
    "c3=Dense(onehot_label_train.shape[1])(c2)\n",
    "c4=Activation(\"softmax\")(c3)\n",
    "\n",
    "\n",
    "model_vqa = Model(inputs = bert.input+[vgg.input], outputs = c4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### RESNET-50 & LSTM ####################\n",
    "\n",
    "resnet=resnet_image_model()\n",
    "lstm=lstm_text_model(emb_mat,emb_mat.shape[0],padding_train_ques.shape[1])\n",
    "\n",
    "combined = Concatenate(axis=1)([lstm.output,resnet.output])\n",
    "c1=Dense(2048,activation=\"relu\")(combined)\n",
    "c2=Dense(1000,activation=\"relu\")(c1)\n",
    "c3=Dense(onehot_label_train.shape[1])(c2)\n",
    "c4=Activation(\"softmax\")(c3)\n",
    "\n",
    "\n",
    "model_vqa = Model(inputs=[lstm.input,resnet.input], outputs=c4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### RESNET-50 & BERT ####################\n",
    "\n",
    "resnet=resnet_image_model()\n",
    "bert=bert_text_model(12,'/home/local/AD/asarkar2/BERT')\n",
    "\n",
    "combined = Concatenate(axis=1)([bert.output,resnet.output])\n",
    "c1=Dense(2048,activation=\"relu\")(combined)\n",
    "c2=Dense(1000,activation=\"relu\")(c1)\n",
    "c3=Dense(onehot_label_train.shape[1])(c2)\n",
    "c4=Activation(\"softmax\")(c3)\n",
    "\n",
    "\n",
    "model_vqa = Model(inputs = bert.input+[resnet.input], outputs = c4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vqa.compile(loss='categorical_crossentropy', optimizer=\"SGD\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### VGG 16 & LSTM ###########################################################\n",
    "callback = [EarlyStopping(monitor='val_loss',mode=\"min\",verbose=1, patience=40),\n",
    "             ModelCheckpoint('train_{}_{}.h5'.format(\"vgg\",\"bert\"), monitor='val_loss',mode=\"min\" ,verbose=1,save_best_only=True)]\n",
    "fit=model_vqa.fit([padding_train_ques,train_image_std],onehot_label_train,validation_data=([padding_valid_ques,valid_image_std],onehot_label_valid),epochs=300,batch_size=32,verbose=0, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################### VGG 16 & BERT ###########################################################\n",
    "callback = [EarlyStopping(monitor='val_loss',mode=\"min\",verbose=1, patience=40),\n",
    "             ModelCheckpoint('train_{}_{}.h5'.format(\"vgg\",\"bert\"), monitor='val_loss',mode=\"min\" ,verbose=1,save_best_only=True)]\n",
    "fit=model_vqa.fit(train_ques_bert+[train_image_std],onehot_label_train,validation_data=(valid_ques_bert+[valid_image_std],onehot_label_valid),epochs=300,batch_size=32,verbose=0, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample for any LSTM + (VGG/RESNET) model\n",
    "\n",
    "################### load VGG 16 & LSTM model #################################\n",
    "\n",
    "model = load_model('train_vgg_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accu=model.evaluate([padding_train_ques,train_image_std],onehot_label_train)\n",
    "valid_accu=model.evaluate([padding_valid_ques,valid_image_std],onehot_label_valid)\n",
    "test_accu=model.evaluate([padding_test_ques,test_image_std],onehot_label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accu, valid_accu,test_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample for any BERT + (VGG/RESNET) model\n",
    "\n",
    "################### load VGG 16 & BERT model #################################\n",
    "\n",
    "model = load_model('train_vgg_bert.h5',custom_objects=get_custom_objects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accu=model.evaluate(train_ques_bert+[train_image_std],onehot_label_train)\n",
    "valid_accu=model.evaluate(valid_ques_bert+[valid_image_std],onehot_label_valid)\n",
    "test_accu=model.evaluate(test_ques_bert+[test_image_std],onehot_label_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
