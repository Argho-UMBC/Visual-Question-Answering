{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Call Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0508 15:52:49.730612 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/src/deepexplain/deepexplain/tensorflow/methods.py:556: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, merge,TimeDistributed,Concatenate,Bidirectional,BatchNormalization,Add,AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,Lambda, Permute,Reshape,Multiply,multiply\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import metrics\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import optimizers\n",
    "\n",
    "from keras_bert import get_custom_objects\n",
    "from tqdm import tqdm\n",
    "from chardet import detect\n",
    "import keras\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import Tokenizer as k_Tokenizer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,LabelBinarizer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,confusion_matrix,auc\n",
    "from sklearn.utils import class_weight\n",
    "from  sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import os, argparse,cv2,h5py,string\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "import codecs\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage import data\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto,InteractiveSession\n",
    "from keras.backend.tensorflow_backend import set_session,clear_session,get_session\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.per_process_gpu_memory_fraction =.3\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.visible_device_list = \"1\"\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = .3\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "# G =tf.Graph()\n",
    "# sess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))\n",
    "# sess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))\n",
    "\n",
    "\n",
    "\n",
    "# Seed value\n",
    "\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 2019\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "\n",
    "# Declare the two file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/local/AD/asarkar2/Research/Modality'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Call the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This files contains the Image ID, Related Question and Corrospoding answers. Each file contains training data (first 3200), validation data (next 500) and testing data (rest 125)\n",
    "\n",
    "######################## If we do analysis for \"Modality\" category ############################\n",
    "\n",
    "\n",
    "filename_mod=\"/home/local/AD/asarkar2/Question/C1_Modality_all.txt\"\n",
    "filename_glove=\"/home/local/AD/asarkar2/glove.6B.100d.txt\"     ### golve vector\n",
    "\n",
    "# filename_organ=\"/home/local/AD/asarkar2/Question/C3_Organ_all.txt\"\n",
    "# filename_plane=\"/home/local/AD/asarkar2/Question/C2_plane_all.txt\"\n",
    "\n",
    "### This files are uploaded to \"Data file\" in google drive\n",
    "### Data File------> All Question  (Location)\n",
    "\n",
    "####################### Image Path #################################################################\n",
    "\n",
    "### This files are uploaded to \"Data file\" in google drive\n",
    "### Data File------> All Image  (Location)\n",
    "\n",
    "img_path_train=\"/home/local/AD/asarkar2/Image File/VQAMed2019_Train_Images/VQAMed2019_Train_Images\"\n",
    "img_path_valid=\"/home/local/AD/asarkar2/Image File/VQAMed2019_Valid_Images/VQAMed2019_Valid_Images\"\n",
    "img_path_test=\"/home/local/AD/asarkar2/Image File/VQAMed2019_Test_Images/VQAMed2019_Test_Images\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/local/AD/asarkar2/Research/Modality'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Call a \"separate\" function to separate image id, question and answer from above file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model_util.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id=separate(filename_mod).get(\"Image_id\")\n",
    "question=separate(filename_mod).get(\"Question\")\n",
    "answer=separate(filename_mod).get(\"Answer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 3200 elements are from train data\n",
    "# 3200-3700 elements are from valid data\n",
    "# rest are from test data\n",
    "\n",
    "image_id_train=img_id[:3200]\n",
    "image_id_valid=img_id[3200:3700]\n",
    "image_id_test=img_id[3700:]\n",
    "\n",
    "\n",
    "question_train=question[:3200]\n",
    "question_valid=question[3200:3700]\n",
    "question_test=question[3700:]\n",
    "\n",
    "answer_train=answer[:3200]\n",
    "answer_valid=answer[3200:3700]\n",
    "answer_test=answer[3700:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer_valid=[items if items in answer_train else \"unknown\" for items in answer_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer_test=[items if items in answer_train else \"unknown\" for items in answer_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer_train=answer_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call image_preprocess function to process the image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=image_preprocess(img_path_train,image_id_train)\n",
    "valid_image=image_preprocess(img_path_valid,image_id_valid)\n",
    "test_image=image_preprocess(img_path_test,image_id_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image=np.load(\"./Image_array/modality_train_image.npy\")\n",
    "# valid_image=np.load(\"./Image_array/modality_valid_image.npy\")\n",
    "# test_image=np.load(\"./Image_array/modality_test_image.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrp_train=np.load(\"mask_dtd_organ_resnet_lstm_train.npy\")\n",
    "# lrp_valid=np.load(\"mask_dtd_organ_resnet_lstm_valid.npy\")\n",
    "# lrp_test=np.load(\"mask_dtd_organ_resnet_lstm_test.npy\")\n",
    "\n",
    "\n",
    "# lrp_train=np.array(lrp_train)\n",
    "# lrp_valid=np.array(lrp_valid)\n",
    "# lrp_test=np.array(lrp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Standardized the image pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_std=train_image/255\n",
    "valid_image_std=valid_image/255\n",
    "test_image_std=test_image/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Call label_preprocess Class to pre-process the answer (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################# Integer Label Encoding ##########################################\n",
    "\n",
    "integer_label_train=LabelPreprocess.integer_encoding(final_answer_train,final_answer_train)\n",
    "integer_label_valid=LabelPreprocess.integer_encoding(final_answer_valid,final_answer_train)\n",
    "integer_label_test=LabelPreprocess.integer_encoding(final_answer_test,final_answer_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "################# One-Hot Label Encoding ##########################################\n",
    "\n",
    "\n",
    "onehot_label_train=LabelPreprocess.onehot_encoding(final_answer_train,final_answer_train)\n",
    "onehot_label_valid=LabelPreprocess.onehot_encoding(final_answer_valid,final_answer_train)\n",
    "onehot_label_test=LabelPreprocess.onehot_encoding(final_answer_test,final_answer_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BERT model question pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run bert_question_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ques_bert,valid_ques_bert,test_ques_bert=bert_question_preprocess(13,'/home/local/AD/asarkar2/BERT',question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LSTM model question pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## sequence of the token for question ############################################\n",
    "\n",
    "seq_train_question=QuestionPreprocess.sequence_question(question_train,question_train)\n",
    "seq_valid_question=QuestionPreprocess.sequence_question(question_valid,question_train)\n",
    "seq_test_question=QuestionPreprocess.sequence_question(question_test,question_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenght=[]\n",
    "# for i in range(len(seq_train_question)):\n",
    "#     l=seq_train_question[i]\n",
    "#     le=len(l)\n",
    "#     lenght.append(le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Padding question ########################################################\n",
    "\n",
    "# maxlength of each question for modality-----> 11\n",
    "# maxlength of each question for organ-----> 10\n",
    "# maxlength of each question for modality-----> 9\n",
    "\n",
    "\n",
    "padding_train_ques=QuestionPreprocess.padding_question(seq_train_question,maxlength=11,padding_criterion=\"post\")\n",
    "padding_valid_ques=QuestionPreprocess.padding_question(seq_valid_question,maxlength=11,padding_criterion=\"post\")\n",
    "padding_test_ques=QuestionPreprocess.padding_question(seq_test_question,maxlength=11,padding_criterion=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 11)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_test_ques.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Embedding Matrix for Glove Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(glove_filename,token_data): \n",
    "    word_index=token_data # call my_token function\n",
    "    embeddings_index = dict()\n",
    "    f = open(glove_filename,encoding='utf8') # call glove vector text file\n",
    "    for line in f:\n",
    "        values=line.split() # split the each line in text file\n",
    "        word = values[0] # first index associate with word and othe other indexs represent embedding vector associated with that word \n",
    "        coefs = np.asarray(values[1:], dtype='float32') \n",
    "        embeddings_index[word] = coefs\n",
    "#         print(embeddings_index)\n",
    "    f.close()\n",
    "\n",
    "    vocab_size=len(token_data)\n",
    "    embedding_matrix=np.zeros((vocab_size+1,100)) # define embedding matrix\n",
    "#     print(word_index.items())\n",
    "    for word,i in word_index.items():                # create our embedding matrix for each word of questions.\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        elif embedding_vector is None:\n",
    "            embedding_matrix[i] =embeddings_index.get(\"unk\")\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_data=QuestionPreprocess.tokenize_question(question_train)\n",
    "\n",
    "emb_mat=embedding(filename_glove,token_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run vgg_model_mfb.ipynb\n",
    "%run resnet_model_mfb.ipynb\n",
    "%run lstm_text_model.ipynb\n",
    "# %run bert_text_model.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### RESNET-50 & LSTM ####################\n",
    "\n",
    "resnet=resnet_image_model()\n",
    "lstm=lstm_text_model(emb_mat,emb_mat.shape[0],padding_train_ques.shape[1])\n",
    "\n",
    "# combined = Concatenate(axis=1)([lstm.output,resnet.output])\n",
    "# c1=Dense(2048,activation=\"relu\")(combined)\n",
    "# c2=Dense(1000,activation=\"relu\")(c1)\n",
    "# c3=Dense(onehot_label_train.shape[1])(c2)\n",
    "# c4=Activation(\"softmax\")(c3)\n",
    "\n",
    "# model_vqa = Model(inputs=[lstm.input,resnet.input], outputs=c4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "\n",
    "lstm_out=lstm.output\n",
    "\n",
    "\n",
    "'''\n",
    "Question Attention\n",
    "'''\n",
    "\n",
    "qatt_conv1=Convolution2D(2048,(1,1))(lstm.output)\n",
    "qatt_relu = Activation(\"relu\")(qatt_conv1)\n",
    "qatt_conv2 = Convolution2D(2,(1,1)) (qatt_relu) # (N,L,1,2)\n",
    "qatt_conv2=Lambda(lambda x: K.squeeze(x, axis=2))(qatt_conv2)\n",
    "qatt_conv2 = Permute((2,1))(qatt_conv2)\n",
    "\n",
    "qatt_softmax = Activation(\"softmax\")(qatt_conv2)\n",
    "qatt_softmax =Reshape( (11, 1,2))(qatt_softmax)\n",
    "\n",
    "\n",
    "def t_qatt_mask(tensors):\n",
    "#     lstm=lstm_text_model(emb_mat,emb_mat.shape[0],padding_train_ques.shape[1])\n",
    "#     lstm_out=lstm.output\n",
    "    qatt_feature_list = []\n",
    "    ten1=tensors[0]\n",
    "    ten2=tensors[1]\n",
    "\n",
    "    for i in range(2):\n",
    "        t_qatt_mask = ten1[:,:,:,i]  # (N,1,L,1)\n",
    "        t_qatt_mask=K.reshape(t_qatt_mask,(-1,11,1,1))\n",
    "        t_qatt_mask = t_qatt_mask * ten2  # (N,1024,L,1)\n",
    "#         print(t_qatt_mask)\n",
    "    #     t_qatt_mask = K.sum(t_qatt_mask,axis=1,keepdims=True)\n",
    "        t_qatt_mask= K.sum(t_qatt_mask,axis=1,keepdims=True)\n",
    "#         print(t_qatt_mask)\n",
    "\n",
    "        qatt_feature_list.append(t_qatt_mask)\n",
    "    \n",
    "    qatt_feature_concat = K.concatenate(qatt_feature_list)  # (N,2048,1,1)\n",
    "    return qatt_feature_concat\n",
    "\n",
    "\n",
    "q_feat_resh=Lambda(t_qatt_mask)([qatt_softmax,lstm_out])\n",
    "\n",
    "q_feat_resh =Lambda(lambda x: K.squeeze(x, axis=2))(q_feat_resh)\n",
    "\n",
    "q_feat_resh=Permute((2,1))(q_feat_resh)\n",
    "q_feat_resh=Reshape([2048])(q_feat_resh)                                           # (N,2048)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    " MFB Image  with Attention\n",
    "\n",
    "'''\n",
    "\n",
    "# image_feature=vgg.output\n",
    "\n",
    "\n",
    "i_feat_resh = Reshape((9,1, 2048))(resnet.output)  # (N,2048,9)\n",
    "\n",
    "iatt_fc = Dense(5000,activation=\"tanh\")(q_feat_resh)  # (N,5000)\n",
    "iatt_resh = Reshape( (1, 1,5000))(iatt_fc)  # (N,5000,1,1)\n",
    "iatt_conv = Convolution2D(5000,(1,1)) (i_feat_resh) # (N,5000,9,1)\n",
    "\n",
    "iatt_eltwise = multiply([iatt_resh , iatt_conv])  # (N,5000,9,1)\n",
    "iatt_droped = Dropout(0.1)(iatt_eltwise)\n",
    "\n",
    "iatt_permute1 = Permute(( 3 ,2, 1))(iatt_droped)  # (N,9,5000,1)\n",
    "iatt_resh2 = Reshape( (1000, 5,9))(iatt_permute1)\n",
    "iatt_sum =Lambda(lambda x: K.sum(x, axis=2,keepdims=True))(iatt_resh2)\n",
    "iatt_permute2 =Permute((3, 2, 1))(iatt_sum)  # (N,1000,9,1)\n",
    "iatt_sqrt = Lambda(lambda x:K.sqrt(Activation(\"relu\")(x)) - K.sqrt(Activation(\"relu\")(-x)))(iatt_permute2)\n",
    "iatt_sqrt =Reshape([-1])(iatt_sqrt)\n",
    "iatt_l2=Lambda(lambda x: K.l2_normalize(x,axis=1))(iatt_sqrt)\n",
    "iatt_l2=Reshape((9,1,1000))(iatt_l2)\n",
    "\n",
    "\n",
    "iatt_conv1 =Convolution2D (2048,(1,1)) (iatt_l2) # (N,2048,9,1)\n",
    "iatt_relu = Activation(\"relu\")(iatt_conv1)\n",
    "iatt_conv2 = Convolution2D(2,(1,1))(iatt_relu)  # (N,2,9,1)\n",
    "iatt_conv2 = Reshape((2,9) )(iatt_conv2)\n",
    "iatt_softmax = Activation(\"softmax\")(iatt_conv2)\n",
    "iatt_softmax = Reshape((9,1, 2))(iatt_softmax)\n",
    "\n",
    "\n",
    "def iatt_feature_list(tensors):\n",
    "#     global i_feat_resh\n",
    "    ten3=tensors[0]\n",
    "    ten4=tensors[1]\n",
    "    iatt_feature_list = []\n",
    "    for j in range(2):\n",
    "        iatt_mask = ten3[:,:,:,j] # (N,1,9,1)\n",
    "        iatt_mask=K.reshape(iatt_mask,(-1,9,1,1))   \n",
    "        iatt_mask = iatt_mask * ten4  # (N,2048,9,1)\n",
    "#         print(iatt_mask)\n",
    "        iatt_mask = K.sum(iatt_mask,axis=1 ,keepdims=True)\n",
    "        iatt_feature_list.append(iatt_mask)\n",
    "    iatt_feature_cat = K.concatenate(iatt_feature_list)  # (N,1024,1,1)\n",
    "    return iatt_feature_cat\n",
    "\n",
    "iatt_feature_cat=Lambda(iatt_feature_list)([iatt_softmax,i_feat_resh])\n",
    "iatt_feature_cat =Lambda(lambda x: K.squeeze(x, axis=2))(iatt_feature_cat)\n",
    "iatt_feature_cat=Permute((2,1))(iatt_feature_cat)\n",
    "iatt_feature_cat=Reshape([-1])(iatt_feature_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fine-grained Image-Question MFH fusion\n",
    "'''\n",
    "# print(q_feat_resh.shape)\n",
    "# print(bert_encode.shape)\n",
    "\n",
    "# if mode != 'train':\n",
    "#     q_feat_resh = q_feat_resh.unsqueeze(0)\n",
    "\n",
    "mfb_q = Dense(5000, activation=\"tanh\")(q_feat_resh)  # (N,5000)\n",
    "mfb_i = Dense(5000,activation=\"relu\")(iatt_feature_cat)  # (N,5000)\n",
    "mfb_eltwise =multiply([mfb_q, mfb_i])\n",
    "mfb_drop1 = Dropout(0.1)(mfb_eltwise)\n",
    "mfb_resh = Reshape(  (1000,5,1))(mfb_drop1)  # (N,1,1000,5)\n",
    "mfb_sum = Lambda(lambda x: K.sum(x, axis=2, keepdims=True))(mfb_resh)\n",
    "mfb_out = Reshape([1000])(mfb_sum)\n",
    "mfb_sqrt =Lambda(lambda x: K.sqrt(Activation(\"relu\")(x)) - K.sqrt(Activation(\"relu\")(-x)))(mfb_out)\n",
    "\n",
    "# if mode != 'train':\n",
    "#     mfb_sqrt = mfb_sqrt.unsqueeze(0)\n",
    "\n",
    "mfb_l2_1 = Lambda(lambda x: K.l2_normalize(x,axis=1))(mfb_sqrt)\n",
    "\n",
    "mfb_q2 = Dense(5000, activation=\"tanh\")(q_feat_resh) # (N,5000)\n",
    "mfb_i2 = Dense(5000,activation=\"relu\")(iatt_feature_cat)  # (N,5000)\n",
    "mfb_eltwise2 = multiply([mfb_q2, mfb_i2])  # (N,5000)\n",
    "mfb_eltwise2 =multiply([mfb_eltwise2, mfb_drop1])\n",
    "mfb_drop2 = Dropout(0.1)(mfb_eltwise2)\n",
    "mfb_resh2 = Reshape( (1000,5,1))(mfb_drop2)\n",
    "#                              # (N,1,1000,5)\n",
    "mfb_sum2 = Lambda(lambda x: K.sum(x, 2, keepdims=True))(mfb_resh2)\n",
    "mfb_out2 =Reshape([1000])(mfb_sum2)\n",
    "mfb_sqrt2 =Lambda(lambda x: K.sqrt(Activation(\"relu\")(x)) - K.sqrt(Activation(\"relu\")(-x)))(mfb_out2)\n",
    "\n",
    "# if mode != 'train':\n",
    "#     mfb_sqrt2 = mfb_sqrt2.unsqueeze(0)\n",
    "mfb_l2_2 =Lambda(lambda x: K.l2_normalize(x,axis=1))(mfb_sqrt2)\n",
    "\n",
    "mfb_l2_3 = Concatenate(axis=1)([mfb_l2_1, mfb_l2_2])  # (N,2000)\n",
    "prediction = Dense(onehot_label_train.shape[1],activation=\"softmax\")(mfb_l2_3)\n",
    "\n",
    "# # prediction=F.log_softmax(prediction)\n",
    "# # prediction=self.Sigmoid(prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vqa=Model(inputs=[lstm.input,resnet.input],outputs=prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vqa.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01, momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vqa.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(answer[:3700]),\n",
    "#                                                  answer[:3700])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################### VGG 16 & LSTM ###########################################################\n",
    "callback = [EarlyStopping(monitor='val_loss',mode=\"min\",verbose=1, patience=30),\n",
    "             ModelCheckpoint('train_{}_{}_{}_{}.h5'.format(\"resnet\",\"lstm\",\"mfb_coatt\",\"modality\"), monitor='val_loss',mode=\"min\" ,verbose=1,save_best_only=True)]\n",
    "fit=model_vqa.fit([padding_train_ques,train_image_std],onehot_label_train,validation_data=([padding_valid_ques,valid_image_std],onehot_label_valid),epochs=300,batch_size=32,verbose=0, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #################################### VGG 16 & BERT ###########################################################\n",
    "# callback = [EarlyStopping(monitor='val_loss',mode=\"min\",verbose=1, patience=40),\n",
    "#              ModelCheckpoint('train_{}_{}_{}.h5'.format(\"resnet\",\"bert\",\"mod\"), monitor='val_loss',mode=\"min\" ,verbose=1,save_best_only=True)]\n",
    "# fit=model_vqa.fit(train_ques_bert+[train_image_std],onehot_label_train,validation_data=(valid_ques_bert+[valid_image_std],onehot_label_valid),epochs=300,batch_size=32,verbose=0, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot training history\n",
    "plt.plot(fit.history['acc'], label='train')\n",
    "plt.plot(fit.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0508 15:56:59.059647 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0508 15:56:59.090624 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0508 15:56:59.102488 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0508 15:56:59.129834 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0508 15:57:00.306254 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0508 15:57:00.387344 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0508 15:57:08.312241 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "W0508 15:57:08.351406 140587179886400 deprecation.py:506] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0508 15:57:14.780360 140587179886400 deprecation_wrapper.py:119] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0508 15:57:15.240428 140587179886400 deprecation.py:323] From /home/local/AD/psanjay/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "### Sample for any LSTM + (VGG/RESNET) model\n",
    "\n",
    "################### load VGG 16 & LSTM model #################################\n",
    "\n",
    "model = load_model('./3-Model/train_resnet_lstm_mfb_coatt_modality.h5',custom_objects=get_custom_objects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 21s 7ms/step\n",
      "500/500 [==============================] - 3s 6ms/step\n",
      "125/125 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "train_accu=model.evaluate([padding_train_ques,train_image_std],onehot_label_train)\n",
    "valid_accu=model.evaluate([padding_valid_ques,valid_image_std],onehot_label_valid)\n",
    "test_accu=model.evaluate([padding_test_ques,test_image_std],onehot_label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_accu=model.evaluate([padding_train_ques,lrp_train],onehot_label_train)\n",
    "# valid_accu=model.evaluate([padding_valid_ques,lrp_valid],onehot_label_valid)\n",
    "# test_accu=model.evaluate([padding_test_ques,lrp_test],onehot_label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8118198376893997, 0.701875] [1.0001466975212097, 0.6700000009536743] [1.0262185745239258, 0.6799999942779541]\n"
     ]
    }
   ],
   "source": [
    "print(train_accu, valid_accu,test_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6542676526456089 0.671241464202495 0.7444873728838612\n"
     ]
    }
   ],
   "source": [
    "y_pred_train=model.predict([padding_train_ques,train_image_std])\n",
    "y_pred_valid=model.predict([padding_valid_ques,valid_image_std])\n",
    "y_pred_test=model.predict([padding_test_ques,test_image_std])\n",
    "\n",
    "\n",
    "y_pred_valid1=np.argmax(y_pred_valid,axis=1)\n",
    "y_pred_train1=np.argmax(y_pred_train,axis=1)\n",
    "y_pred_test1=np.argmax(y_pred_test,axis=1)\n",
    "\n",
    "# len(y_pred_valid1)\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "\n",
    "ruc_train=multiclass_roc_auc_score(integer_label_train, y_pred_train1, average=\"macro\")\n",
    "ruc_valid=multiclass_roc_auc_score(integer_label_valid, y_pred_valid1, average=\"macro\")\n",
    "ruc_test=multiclass_roc_auc_score(integer_label_test,y_pred_test1, average=\"macro\")\n",
    "\n",
    "print(ruc_train,ruc_valid,ruc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Recall, Precision & F-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_true, y_pred, y_score=None, average='micro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true %s is not the same shape as y_pred %s\" % (\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='weighted'))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred'] = pred_cnt\n",
    "    class_report_df['pred'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_it, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_it])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report(integer_label_test, y_pred_test1, y_score=None, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
